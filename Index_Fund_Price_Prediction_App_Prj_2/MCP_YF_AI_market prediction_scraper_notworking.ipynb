{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using YF and other sources. create a market analysis and prediction application for Index funds using the following methods:\n",
    "    - nueral networks and Deep learning\n",
    "    - natural language processing\n",
    "    - transformers\n",
    "    - Langchain\n",
    "    - API chat\n",
    "    - converssational memory\n",
    "    - event planning\n",
    "\n",
    "#### set up your on conda envoiroment.\n",
    "#### Acquire all data and begin analysis.\n",
    "    -make suggestions on data retrieval.\n",
    "    https://archive.ics.uci.edu/\n",
    "    https://www.data.world/\n",
    "    https://www.kaggle.com/\n",
    "    https://www.data.gov/\n",
    "    https://github.com/awesomedata/awesome-public-datasets\n",
    "\n",
    "\n",
    "#### what methods to use?\n",
    "#### Finalize your strategy for incorporating pre-trained transformers, OpenAI, or other technologies into your solution.\n",
    "#### list of links for documentation of methodology:\n",
    "https://playground.tensorflow.org/\n",
    "https://www.tensorflow.org/api_docs/python/tf\n",
    "https://keras.io/api/keras_tuner/hyperparameters/\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "https://docs.python.org/3/library/pickle.html\n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "https://www.nltk.org/book/\n",
    "https://spacy.io/\n",
    "https://pypi.org/project/gensim/\n",
    "https://dataaspirant.com/gensim-guide-topic-modeling/\n",
    "https://huggingface.co/\n",
    "https://openai.com/research/whisper\n",
    "https://python.langchain.com/docs/get_started/introduction\n",
    "https://openai.com/\n",
    "\n",
    "#### requirements:\n",
    "For the Final Project, you will work with your group to collaboratively solve or analyze a problem using advanced ML methodologies. In your solution, you will incorporate transformer models, natural language processing (NLP) techniques, and other tools acquired throughout the course, in addition to at least one new technology that we haven’t covered together.\n",
    "\n",
    "Here are the specific requirements:\n",
    "\n",
    "Identify a problem worth solving or analyzing.\n",
    "Find a dataset or datasets that are sufficiently large enough to effectively train a ML model or neural network with a high degree of accuracy to ensure that your results are reliable.\n",
    "Evaluate the trained model(s) using testing data. Include any calculations, metrics, or visualizations needed to evaluate the performance.\n",
    "You must use at least two of the following:\n",
    "scikit-learn\n",
    "Keras\n",
    "TensorFlow\n",
    "Hugging Face\n",
    "spaCy or Natural Language Toolkit (NLTK)\n",
    "LangChain\n",
    "OpenAI\n",
    "You must use one additional library or technology NOT covered in class, such as:\n",
    "Valence Aware Dictionary for Sentiment Reasoning (VADER)\n",
    "Whisper (OpenAI’s automatic speech recognition system)\n",
    "DALL·E (OpenAI’s text-to-image model)\n",
    "Other OpenAI capabilities, including:\n",
    "Text-to-speech\n",
    "GPT-4 with vision (GPT-4V)\n",
    "PyTorch\n",
    "For this project, you can focus your efforts within a specific industry, as detailed in the following examples.\n",
    "\n",
    "Finance\n",
    "Build a customer service chatbot for a financial firm that analyzes a user’s request and makes customized recommendations in one or more languages.\n",
    "\n",
    "Develop a deep learning model that forecasts and predicts stock prices for at least three publicly traded companies.\n",
    "\n",
    "Use NLP, transformers, or OpenAI to summarize key takeaways from a company’s earnings call.\n",
    "Model Implementation (25 points)\n",
    "There is a Jupyter notebook that thoroughly describes the data extraction, cleaning, preprocessing, and transformation process, and the cleaned data is exported as CSV files for a machine or deep learning model, or NLP application. (10 points)\n",
    "\n",
    "A Python script initializes, trains, and evaluates a model or loads a pre-trained model. (10 points)\n",
    "\n",
    "At least one additional library or technology NOT covered in class is used. (5 points)\n",
    "\n",
    "Model Optimization (25 points)\n",
    "The model optimization and evaluation process showing iterative changes made to the model and the resulting changes in model performance is documented in either a CSV/Excel table or in the Python script itself. (15 points)\n",
    "\n",
    "Overall model performance is printed or displayed at the end of the script. (10 points)\n",
    "\n",
    "GitHub Documentation (25 points)\n",
    "GitHub repository is free of unnecessary files and folders and has an appropriate .gitignore in use. (10 points)\n",
    "\n",
    "The README is customized as a polished presentation of the content of the project. (15 points)\n",
    "\n",
    "Presentation Requirements (25 points)\n",
    "Your presentation should cover the following:\n",
    "\n",
    "An executive summary or overview of the project and project goals. (5 points)\n",
    "\n",
    "An overview of the data collection, cleanup, and exploration processes. Include a description of how you evaluated the trained model(s) using testing data. (5 points)\n",
    "\n",
    "The approach that your group took to achieve the project goals. (5 points)\n",
    "\n",
    "Any additional questions that surfaced, what your group might research next if more time was available, or a plan for future development. (3 points)\n",
    "\n",
    "The results and conclusions of the application or analysis. (3 points)\n",
    "\n",
    "Slides that effectively demonstrate the project. (2 points)\n",
    "\n",
    "Slides that are visually clean and professional. (2 points)\n",
    "\n",
    "#### IMPORTANT\n",
    "Whenever you use a dataset or create a new dataset based on other sources (such as existing datasets or information scraped from websites), make sure to use the following guidelines:\n",
    "\n",
    "Check for copyright protections, and make sure that the way you plan to use this dataset is within the bounds of fair use.\n",
    "\n",
    "Document how you intend to use this dataset now and in the future. Find any licenses or terms of use associated with the dataset, and review them to confirm that your intended use is in compliance.\n",
    "\n",
    "Investigate how the dataset was collected. Identify any indicators that the data was obtained from a source that the compilers were not authorized to access.\n",
    "\n",
    "You’ll likely have to adjust your project plan as you explore the available data. That’s okay! This is all part of the process. Just make sure that everyone in the group is aligned on the project’s goals as you make changes.\n",
    "\n",
    "Make sure that your datasets are not too large for your personal computer. Big datasets are difficult to manage locally, so consider using data subsets or different datasets altogether.\n",
    "\n",
    "Data Cleanup and Analysis\n",
    "Now that you’ve picked your data, it’s time to tackle development and analysis. This is where the fun starts!\n",
    "\n",
    "The analysis process can be broken into two broad phases: (1) Exploration and cleanup, and (2) analysis.\n",
    "\n",
    "As you’ve learned, you’ll need to explore, clean, and reformat your data before you can begin answering your research questions. We recommend keeping track of these exploration and cleanup steps in a dedicated Jupyter notebook to stay organized and make it easier to present your work later.\n",
    "\n",
    "After you’ve cleaned your data and are ready to start crunching numbers, you should track your work in a Jupyter notebook dedicated specifically to analysis. We recommend focusing your analysis on multiple techniques, such as aggregation, correlation, comparison, summary statistics, sentiment analysis, and time-series analysis. Don’t forget to include plots during both the exploration and analysis phases. Creating plots along the way can reveal insights and interesting trends in the data that you might not notice if you wait until you’re preparing for your presentation. Presentation requirements will be further explained in the next module.\n",
    "\n",
    "Presentation Guidelines\n",
    "This section lists the Final Project presentation guidelines. Each group will prepare a formal, 10-minute presentation (7 minutes for the presentation followed by a 3-minute Q&A session) that covers the following points.\n",
    "\n",
    "An executive summary or overview of the project and project goals:\n",
    "\n",
    "Explain how the project relates to the industry you selected.\n",
    "An overview of the data collection, cleanup, and exploration processes:\n",
    "\n",
    "Describe the source of your data and why you chose it for your project.\n",
    "\n",
    "Describe the collection, cleanup, and exploration processes.\n",
    "\n",
    "The approach that your group took to achieve the project goals:\n",
    "\n",
    "Include any relevant code or demonstrations of the application or analysis.\n",
    "\n",
    "Discuss any unanticipated insights or problems that arose and how you resolved them.\n",
    "\n",
    "The results or conclusions of the application or analysis:\n",
    "\n",
    "Include relevant images or examples to support your work.\n",
    "\n",
    "If the project goal was not achieved, discuss the issues and how you attempted to resolve them.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "Briefly discuss potential next steps for the project.\n",
    "It’s crucial that you find time to rehearse before presentation day.\n",
    "\n",
    "On the day of your presentation, each member of your group is required to submit the URL of your GitHub repository for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Project Planning and Design\n",
    "Define Goals and Scope: Clearly define what your index fund prediction application aims to achieve, including the specific functionalities you want to implement.\n",
    "- Technology Stack: Decide on the technologies and frameworks you will use. Based on your requirements, these may include:\n",
    "- Frontend: WIX for the website.\n",
    "- Backend: Python for machine learning and data processing.\n",
    "- AI and ML Libraries: TensorFlow, Keras, spaCy, Hugging Face Transformers, LangChain, Prophet.\n",
    "- Additional Tools: OpenAI's GPT-4 for the chatbot.\n",
    "- Databases: Choose a database for storing and retrieving your data efficiently.\n",
    "\n",
    "#### Step 2: Environment Setup\n",
    "Set Up Development Environment: Create a Conda environment to manage your project's dependencies.\n",
    "\n",
    "\n",
    "-  conda create -n indexfundapp python=3.8\n",
    "-  conda activate indexfundapp\n",
    "\n",
    "Install Required Libraries: \n",
    "-  Install necessary Python libraries within your Conda environment.\n",
    "-  pip install tensorflow keras spacy nltk gensim prophet langchain huggingface_hub\n",
    "\n",
    "\n",
    "#### Step 3: Data Acquisition and Preprocessing\n",
    "- Gather Data: Use APIs and datasets from suggested sources like YFinance, AlphaVantageAPI, and data repositories such as Kaggle, UCI, or Data.gov.\n",
    "- Data Cleanup: Use Python and Pandas to clean and preprocess your data. This might involve handling missing values, normalizing data, and converting data formats.\n",
    "- Data Storage: Store the cleaned data in a database or as CSV files for easy retrieval during analysis.\n",
    "\n",
    "#### Step 4: Model Development and Training\n",
    "- Feature Engineering: Extract features that are relevant for predicting index fund performance.\n",
    "- Model Training: Develop and train your models using deep learning and machine learning techniques. \n",
    "- You might use: Neural networks for complex pattern recognition.\n",
    "- Prophet for time-series forecasting.\n",
    "- Model Evaluation: Validate your models using appropriate metrics such as MSE (Mean Squared Error), MAE (Mean Absolute Error), or others relevant to your prediction tasks.\n",
    "\n",
    "#### Step 5: Integrating AI Technologies\n",
    "- NLP and Transformers: Utilize NLP and transformers for analyzing textual data such as market news or financial reports to augment your predictions.\n",
    "- LangChain for Chatbot: Implement a chatbot using LangChain that leverages GPT-4 to interact with users, providing insights and predictions based on user queries.\n",
    "- Conversational Memory and Event Planning: Design the chatbot to remember past interactions and provide event-based notifications or suggestions.\n",
    "#### Step 6: Application Development\n",
    "- Web Application: Develop the user interface using WIX where users can input data, view predictions, and interact with the chatbot.\n",
    "- API Integration: Ensure your backend Python application can communicate with the frontend via APIs. \n",
    "- Use Flask or Django to create these APIs if WIX's backend capabilities are insufficient.\n",
    "\n",
    "#### Step 7: Testing and Optimization\n",
    "- Testing: Conduct thorough testing of both the machine learning models and the web application to ensure reliability and usability.\n",
    "- Optimization: Fine-tune the performance of your models and the responsiveness of your application.\n",
    "\n",
    "#### Step 8: Deployment\n",
    "- Deploy Web Application: Launch your application on WIX, ensuring that it is accessible to users and that the backend services are properly configured and secured.\n",
    "- Monitor Performance: After deployment, monitor the application's performance and user feedback to identify areas for improvement.\n",
    "\n",
    "#### Step 9: Documentation and Presentation\n",
    "- GitHub Documentation: Maintain a well-documented GitHub repository with your code, libraries used, project setup instructions, and a comprehensive README.\n",
    "- Prepare Presentation: Develop slides and a presentation summarizing your project's goals, methodology, results, and future work. Practice the presentation to ensure clarity and conciseness.\n",
    "\n",
    "#### Step 10: Review and Iterate\n",
    "- Feedback and Iteration: Gather feedback from users and stakeholders. Use this feedback to refine your application and models.\n",
    "- Future Enhancements: Plan for future enhancements such as adding more features, improving model accuracy, or expanding the scope of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --index-url=https://bcms.bloomberg.com/pip/simple blpapi\n",
    "# !pip install beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a session of the Bloomberg Desktop API\n",
    "# session = blpapi.Session()\n",
    "# if not session.start():\n",
    "#     print(\"Failed to start session.\")\n",
    "#     exit()\n",
    "\n",
    "# if not session.openService(\"//blp/refdata\"):\n",
    "#     print(\"Failed to open //blp/refdata\")\n",
    "#     session.stop()\n",
    "#     exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a session of the Bloomberg Desktop API\n",
    "# session = blpapi.Session()\n",
    "# if not session.start():\n",
    "#     print(\"Failed to start session.\")\n",
    "#     exit()\n",
    "\n",
    "# if not session.openService(\"//blp/refdata\"):\n",
    "#     print(\"Failed to open //blp/refdata\")\n",
    "#     session.stop()\n",
    "#     exit()\n",
    "    \n",
    "# # Creating a reference data request\n",
    "# refDataService = session.getService(\"//blp/refdata\")\n",
    "# request = refDataService.createRequest(\"HistoricalDataRequest\")\n",
    "# request.set(\"securities\", \"IBM US Equity\")\n",
    "# request.set(\"fields\", \"PX_LAST\")\n",
    "# request.set(\"startDate\", \"20230101\")\n",
    "# request.set(\"endDate\", \"20230401\")\n",
    "# request.set(\"periodicityAdjustment\", \"ACTUAL\")\n",
    "# request.set(\"periodicitySelection\", \"MONTHLY\")\n",
    "# request.set(\"maxDataPoints\", 100)\n",
    "\n",
    "# # Send the request\n",
    "# print(\"Sending Request:\", request)\n",
    "# session.sendRequest(request)\n",
    "\n",
    "# # Process received events\n",
    "# while(True):\n",
    "#     event = session.nextEvent()\n",
    "#     if event.eventType() == blpapi.Event.PARTIAL_RESPONSE or event.eventType() == blpapi.Event.RESPONSE:\n",
    "#         for msg in event:\n",
    "#             print(msg)\n",
    "#     if event.eventType() == blpapi.Event.RESPONSE:\n",
    "#         break\n",
    "\n",
    "# # Terminate the session\n",
    "# session.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(files)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(download_and_extract(url[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mdownload_and_extract\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(url):\n\u001b[0;32m     18\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 19\u001b[0m     zip_file \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZipFile(BytesIO(response\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[0;32m     20\u001b[0m     files\u001b[38;5;241m=\u001b[39mzip_file\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(files)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\zipfile.py:1302\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1302\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_RealGetContents()\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\zipfile.py:1369\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Data from the SEC.\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import sqlite3\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URLs of the pages with the datasets\n",
    "url = [\n",
    "    \"https://www.sec.gov/dera/data/financial-statement-data-sets\",\n",
    "    \"https://www.sec.gov/dera/data/financial-statement-and-notes-data-set\"\n",
    "]\n",
    "# Function to download and extract zip files # add file path & return the file\n",
    "\n",
    "def download_and_extract(url):\n",
    "    response = requests.get(url)\n",
    "    zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "    files=zip_file.extractall(\"data\")\n",
    "    print(files)\n",
    "    return files\n",
    "print(download_and_extract(url[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the SEC.\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import sqlite3\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URLs of the pages with the datasets\n",
    "urls = [\n",
    "    \"https://www.sec.gov/dera/data/financial-statement-data-sets\",\n",
    "    #\"https://www.sec.gov/dera/data/financial-statement-and-notes-data-set\"\n",
    "]\n",
    "# Function to download and extract zip files # add file path & return the file\n",
    "\n",
    "def download_and_extract(url):\n",
    "    response = requests.get(url)\n",
    "    zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "    files=zip_file.extractall(\"data\")\n",
    "    print(files)\n",
    "    return files    \n",
    "# # Function to scrape the URLs and download the zip files\n",
    "def scrape_and_download():\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Assuming the zip files are linked directly in <a> tags \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            if link['href'].endswith('.zip'):\n",
    "                download_link = url + link['href']\n",
    "                download_and_extract(download_link)\n",
    "# # Function to parse the data files\n",
    "def parse_data_file(file_path):\n",
    "    data_rows = []\n",
    "    if file_path.endswith('.tsv'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file, delimiter='\\t')\n",
    "            next(reader)  # Skip the header row if there is one\n",
    "            for row in reader:\n",
    "                data_rows.append(row)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                data_rows.append([line.strip()])\n",
    "    elif file_path.endswith('.md'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "#             data_rows.append([content])  # store the whole markdown file as a single entry\n",
    "    return data_rows\n",
    "\n",
    "# # Function to create a SQLite database\n",
    "def create_database():\n",
    "    conn = sqlite3.connect('sec_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    # Create tables as necessary, assuming a structure\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS financial_data (id INTEGER PRIMARY KEY, data TEXT)''')\n",
    "    return conn\n",
    "# # Function to populate the database\n",
    "def populate_database(conn, file_path):\n",
    "    cursor = conn.cursor()\n",
    "    # Assuming you have a function to parse files and return rows of data\n",
    "    data_rows = parse_data_file(file_path)\n",
    "    cursor.executemany('INSERT INTO financial_data (data) VALUES (?)', data_rows)\n",
    "    conn.commit()\n",
    "# Calls the functions to scrape, download, parse, and populate the database\n",
    "def main():\n",
    "    scrape_and_download()\n",
    "    conn = create_database()\n",
    "    # Loop through files in the data directory\n",
    "    for root, dirs, files in os.walk(\"data\"):\n",
    "        for file in files:\n",
    "            populate_database(conn, os.path.join(root, file))\n",
    "    conn.close()\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('sec_data.db')\n",
    "\n",
    "# Create a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query the database\n",
    "cursor.execute(\"SELECT * FROM financial_data LIMIT 10;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: financial_data\n",
      "Number of records in financial_data: 0\n"
     ]
    }
   ],
   "source": [
    "# check database content\n",
    "import sqlite3\n",
    "\n",
    "def check_database_content(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check for the existence of tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    if not tables:\n",
    "        print(\"No tables found in the database.\")\n",
    "    else:\n",
    "        for table in tables:\n",
    "            print(f\"Table: {table[0]}\")\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table[0]}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"Number of records in {table[0]}: {count}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# Provide the path to your database file\n",
    "check_database_content('sec_data.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data insertion:\n",
    "def populate_database(conn, file_path):\n",
    "    cursor = conn.cursor()\n",
    "    data_rows = parse_data_file(file_path)\n",
    "    if data_rows:\n",
    "        cursor.executemany('INSERT INTO financial_data (data) VALUES (?)', data_rows)\n",
    "        conn.commit()\n",
    "        print(f\"Inserted {cursor.rowcount} rows from {file_path}\")\n",
    "    else:\n",
    "        print(f\"No data parsed from {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data retrieved from the database.\n"
     ]
    }
   ],
   "source": [
    "#check query execution:\n",
    "import sqlite3\n",
    "\n",
    "def query_data(db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT * FROM financial_data LIMIT 10;\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        if rows:\n",
    "            for row in rows:\n",
    "                print(row)\n",
    "        else:\n",
    "            print(\"No data retrieved from the database.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "query_data('sec_data.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_file(file_path):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    data_rows = []\n",
    "    try:\n",
    "        if file_path.endswith('.tsv'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                reader = csv.reader(file, delimiter='\\t')\n",
    "                headers = next(reader)  # Skip the header row\n",
    "                print(f\"Headers: {headers}\")  # Verify headers\n",
    "                for row in reader:\n",
    "                    data_rows.append(tuple(row))\n",
    "                    if len(data_rows) < 5:  # Print first 5 rows to check\n",
    "                        print(row)\n",
    "        # Repeat similar checks for other file formats\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "    return data_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def parse_data_file(file_path):\n",
    "    print(f\"Checking file: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    data_rows = []\n",
    "    try:\n",
    "        if file_path.endswith('.tsv'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                reader = csv.reader(file, delimiter='\\t')\n",
    "                headers = next(reader)  # Attempt to read headers\n",
    "                print(f\"Headers: {headers}\")  # Verify headers\n",
    "                for row in reader:\n",
    "                    data_rows.append(tuple(row))\n",
    "                    if len(data_rows) < 5:  # Print first 5 rows to check\n",
    "                        print(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "    return data_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing file read for: path/to/your/datafile.tsv\n",
      "File does not exist.\n"
     ]
    }
   ],
   "source": [
    "def test_file_read(file_path):\n",
    "    print(f\"Testing file read for: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File does not exist.\")\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for _ in range(5):  # Read first 5 lines\n",
    "                print(file.readline().strip())\n",
    "\n",
    "# Use an absolute path or adjust the relative path as necessary\n",
    "test_file_read('path/to/your/datafile.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
