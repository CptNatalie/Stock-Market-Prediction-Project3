{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using YF and other sources. create a market analysis and prediction application for Index funds using the following methods:\n",
    "    - nueral networks and Deep learning\n",
    "    - natural language processing\n",
    "    - transformers\n",
    "    - Langchain\n",
    "    - API chat\n",
    "    - converssational memory\n",
    "    - event planning\n",
    "\n",
    "#### set up your on conda envoiroment.\n",
    "#### Acquire all data and begin analysis.\n",
    "    -make suggestions on data retrieval.\n",
    "    https://archive.ics.uci.edu/\n",
    "    https://www.data.world/\n",
    "    https://www.kaggle.com/\n",
    "    https://www.data.gov/\n",
    "    https://github.com/awesomedata/awesome-public-datasets\n",
    "\n",
    "\n",
    "#### what methods to use?\n",
    "#### Finalize your strategy for incorporating pre-trained transformers, OpenAI, or other technologies into your solution.\n",
    "#### list of links for documentation of methodology:\n",
    "https://playground.tensorflow.org/\n",
    "https://www.tensorflow.org/api_docs/python/tf\n",
    "https://keras.io/api/keras_tuner/hyperparameters/\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "https://docs.python.org/3/library/pickle.html\n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "https://www.nltk.org/book/\n",
    "https://spacy.io/\n",
    "https://pypi.org/project/gensim/\n",
    "https://dataaspirant.com/gensim-guide-topic-modeling/\n",
    "https://huggingface.co/\n",
    "https://openai.com/research/whisper\n",
    "https://python.langchain.com/docs/get_started/introduction\n",
    "https://openai.com/\n",
    "\n",
    "#### requirements:\n",
    "For the Final Project, you will work with your group to collaboratively solve or analyze a problem using advanced ML methodologies. In your solution, you will incorporate transformer models, natural language processing (NLP) techniques, and other tools acquired throughout the course, in addition to at least one new technology that we haven’t covered together.\n",
    "\n",
    "Here are the specific requirements:\n",
    "\n",
    "Identify a problem worth solving or analyzing.\n",
    "Find a dataset or datasets that are sufficiently large enough to effectively train a ML model or neural network with a high degree of accuracy to ensure that your results are reliable.\n",
    "Evaluate the trained model(s) using testing data. Include any calculations, metrics, or visualizations needed to evaluate the performance.\n",
    "You must use at least two of the following:\n",
    "scikit-learn\n",
    "Keras\n",
    "TensorFlow\n",
    "Hugging Face\n",
    "spaCy or Natural Language Toolkit (NLTK)\n",
    "LangChain\n",
    "OpenAI\n",
    "You must use one additional library or technology NOT covered in class, such as:\n",
    "Valence Aware Dictionary for Sentiment Reasoning (VADER)\n",
    "Whisper (OpenAI’s automatic speech recognition system)\n",
    "DALL·E (OpenAI’s text-to-image model)\n",
    "Other OpenAI capabilities, including:\n",
    "Text-to-speech\n",
    "GPT-4 with vision (GPT-4V)\n",
    "PyTorch\n",
    "For this project, you can focus your efforts within a specific industry, as detailed in the following examples.\n",
    "\n",
    "Finance\n",
    "Build a customer service chatbot for a financial firm that analyzes a user’s request and makes customized recommendations in one or more languages.\n",
    "\n",
    "Develop a deep learning model that forecasts and predicts stock prices for at least three publicly traded companies.\n",
    "\n",
    "Use NLP, transformers, or OpenAI to summarize key takeaways from a company’s earnings call.\n",
    "Model Implementation (25 points)\n",
    "There is a Jupyter notebook that thoroughly describes the data extraction, cleaning, preprocessing, and transformation process, and the cleaned data is exported as CSV files for a machine or deep learning model, or NLP application. (10 points)\n",
    "\n",
    "A Python script initializes, trains, and evaluates a model or loads a pre-trained model. (10 points)\n",
    "\n",
    "At least one additional library or technology NOT covered in class is used. (5 points)\n",
    "\n",
    "Model Optimization (25 points)\n",
    "The model optimization and evaluation process showing iterative changes made to the model and the resulting changes in model performance is documented in either a CSV/Excel table or in the Python script itself. (15 points)\n",
    "\n",
    "Overall model performance is printed or displayed at the end of the script. (10 points)\n",
    "\n",
    "GitHub Documentation (25 points)\n",
    "GitHub repository is free of unnecessary files and folders and has an appropriate .gitignore in use. (10 points)\n",
    "\n",
    "The README is customized as a polished presentation of the content of the project. (15 points)\n",
    "\n",
    "Presentation Requirements (25 points)\n",
    "Your presentation should cover the following:\n",
    "\n",
    "An executive summary or overview of the project and project goals. (5 points)\n",
    "\n",
    "An overview of the data collection, cleanup, and exploration processes. Include a description of how you evaluated the trained model(s) using testing data. (5 points)\n",
    "\n",
    "The approach that your group took to achieve the project goals. (5 points)\n",
    "\n",
    "Any additional questions that surfaced, what your group might research next if more time was available, or a plan for future development. (3 points)\n",
    "\n",
    "The results and conclusions of the application or analysis. (3 points)\n",
    "\n",
    "Slides that effectively demonstrate the project. (2 points)\n",
    "\n",
    "Slides that are visually clean and professional. (2 points)\n",
    "\n",
    "#### IMPORTANT\n",
    "Whenever you use a dataset or create a new dataset based on other sources (such as existing datasets or information scraped from websites), make sure to use the following guidelines:\n",
    "\n",
    "Check for copyright protections, and make sure that the way you plan to use this dataset is within the bounds of fair use.\n",
    "\n",
    "Document how you intend to use this dataset now and in the future. Find any licenses or terms of use associated with the dataset, and review them to confirm that your intended use is in compliance.\n",
    "\n",
    "Investigate how the dataset was collected. Identify any indicators that the data was obtained from a source that the compilers were not authorized to access.\n",
    "\n",
    "You’ll likely have to adjust your project plan as you explore the available data. That’s okay! This is all part of the process. Just make sure that everyone in the group is aligned on the project’s goals as you make changes.\n",
    "\n",
    "Make sure that your datasets are not too large for your personal computer. Big datasets are difficult to manage locally, so consider using data subsets or different datasets altogether.\n",
    "\n",
    "Data Cleanup and Analysis\n",
    "Now that you’ve picked your data, it’s time to tackle development and analysis. This is where the fun starts!\n",
    "\n",
    "The analysis process can be broken into two broad phases: (1) Exploration and cleanup, and (2) analysis.\n",
    "\n",
    "As you’ve learned, you’ll need to explore, clean, and reformat your data before you can begin answering your research questions. We recommend keeping track of these exploration and cleanup steps in a dedicated Jupyter notebook to stay organized and make it easier to present your work later.\n",
    "\n",
    "After you’ve cleaned your data and are ready to start crunching numbers, you should track your work in a Jupyter notebook dedicated specifically to analysis. We recommend focusing your analysis on multiple techniques, such as aggregation, correlation, comparison, summary statistics, sentiment analysis, and time-series analysis. Don’t forget to include plots during both the exploration and analysis phases. Creating plots along the way can reveal insights and interesting trends in the data that you might not notice if you wait until you’re preparing for your presentation. Presentation requirements will be further explained in the next module.\n",
    "\n",
    "Presentation Guidelines\n",
    "This section lists the Final Project presentation guidelines. Each group will prepare a formal, 10-minute presentation (7 minutes for the presentation followed by a 3-minute Q&A session) that covers the following points.\n",
    "\n",
    "An executive summary or overview of the project and project goals:\n",
    "\n",
    "Explain how the project relates to the industry you selected.\n",
    "An overview of the data collection, cleanup, and exploration processes:\n",
    "\n",
    "Describe the source of your data and why you chose it for your project.\n",
    "\n",
    "Describe the collection, cleanup, and exploration processes.\n",
    "\n",
    "The approach that your group took to achieve the project goals:\n",
    "\n",
    "Include any relevant code or demonstrations of the application or analysis.\n",
    "\n",
    "Discuss any unanticipated insights or problems that arose and how you resolved them.\n",
    "\n",
    "The results or conclusions of the application or analysis:\n",
    "\n",
    "Include relevant images or examples to support your work.\n",
    "\n",
    "If the project goal was not achieved, discuss the issues and how you attempted to resolve them.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "Briefly discuss potential next steps for the project.\n",
    "It’s crucial that you find time to rehearse before presentation day.\n",
    "\n",
    "On the day of your presentation, each member of your group is required to submit the URL of your GitHub repository for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
